{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Persistent Matmul\nThis script demonstrates persistent kernel implementations of matrix multiplication using Triton.\nVarious matmul methods are included, such as naive, persistent, and TMA (Tensor Memory Accelerator) based approaches.\nThe kernels support both FP16 and FP8 data types but the FP8 implementation is only available on CUDA devices with compute capability >= 9.0.\n\nTriton and cuBLAS implementations are benchmarked under different configurations and evaluated using the proton profiler.\nUsers can pass command-line arguments to specify matrix dimensions and iteration steps flexibly.\n\n```bash\n# FP8\npython 09-persistent-matmul.py --prec fp8 --K_range 128 1024 --K_step 128\n\n# FP16\npython 09-persistent-matmul.py --prec fp16 --K_range 128 1024 --K_step 128\n```\nNote that currently this tutorial will fail on devices with a small shared memory size, such as RTX-4090.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import argparse\nimport time\n\nimport torch\nimport triton\nimport triton.language as tl\nimport triton.tools.experimental_descriptor\nimport triton.profiler as proton\n\nif torch.cuda.is_available():\n    from triton._C.libtriton import nvidia\n    cublas_workspace = torch.empty(32 * 1024 * 1024, device=\"cuda\", dtype=torch.uint8)\n    cublas = nvidia.cublas.CublasLt(cublas_workspace)\nelse:\n    cublas = None\n\n\ndef is_cuda():\n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n\n\ndef supports_tma():\n    return is_cuda() and torch.cuda.get_device_capability()[0] >= 9\n\n\ndef _matmul_launch_metadata(grid, kernel, args):\n    ret = {}\n    M, N, K = args[\"M\"], args[\"N\"], args[\"K\"]\n    ret[\"name\"] = f\"{kernel.name} [M={M}, N={N}, K={K}]\"\n    if \"c_ptr\" in args:\n        bytes_per_elem = args[\"c_ptr\"].element_size()\n    else:\n        bytes_per_elem = 1 if args[\"FP8_OUTPUT\"] else 2\n    ret[f\"flops{bytes_per_elem * 8}\"] = 2. * M * N * K\n    ret[\"bytes\"] = bytes_per_elem * (M * K + N * K + M * N)\n    return ret\n\n\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel(a_ptr, b_ptr, c_ptr,  #\n                  M, N, K,  #\n                  stride_am, stride_ak,  #\n                  stride_bk, stride_bn,  #\n                  stride_cm, stride_cn,  #\n                  BLOCK_SIZE_M: tl.constexpr,  #\n                  BLOCK_SIZE_N: tl.constexpr,  #\n                  BLOCK_SIZE_K: tl.constexpr,  #\n                  GROUP_SIZE_M: tl.constexpr,  #\n                  ):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n\n    offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)\n    offs_am = tl.where(offs_am < M, offs_am, 0)\n    offs_bn = tl.where(offs_bn < N, offs_bn, 0)\n\n    offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)\n    offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if (c_ptr.dtype.element_ty == tl.float8e4nv):\n        c = accumulator.to(tl.float8e4nv)\n    else:\n        c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n\ndef matmul(a, b):\n    configs = {\n        torch.float8_e4m3fn: {\n            \"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 128, \"GROUP_SIZE_M\": 8, \"num_stages\": 4,\n            \"num_warps\": 8\n        }, torch.float16: {\n            \"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 64, \"GROUP_SIZE_M\": 8, \"num_stages\": 3,\n            \"num_warps\": 8\n        }\n    }\n    # Check constraints.\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n    M, K = a.shape\n    K, N = b.shape\n    dtype = a.dtype\n\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    # 1D launch kernel where each block gets its own program.\n    grid = lambda META: (triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"]), )\n    matmul_kernel[grid](\n        a, b, c,  #\n        M, N, K,  #\n        a.stride(0), a.stride(1),  #\n        b.stride(0), b.stride(1),  #\n        c.stride(0), c.stride(1),  #\n        BLOCK_SIZE_M=configs[dtype][\"BLOCK_SIZE_M\"],  #\n        BLOCK_SIZE_N=configs[dtype][\"BLOCK_SIZE_N\"],  #\n        BLOCK_SIZE_K=configs[dtype][\"BLOCK_SIZE_K\"],  #\n        GROUP_SIZE_M=configs[dtype][\"GROUP_SIZE_M\"],  #\n        num_stages=configs[dtype][\"num_stages\"],  #\n        num_warps=configs[dtype][\"num_warps\"],  #\n    )\n    return c\n\n\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel_persistent(a_ptr, b_ptr, c_ptr,  #\n                             M, N, K,  #\n                             stride_am, stride_ak,  #\n                             stride_bk, stride_bn,  #\n                             stride_cm, stride_cn,  #\n                             BLOCK_SIZE_M: tl.constexpr,  #\n                             BLOCK_SIZE_N: tl.constexpr,  #\n                             BLOCK_SIZE_K: tl.constexpr,  #\n                             GROUP_SIZE_M: tl.constexpr,  #\n                             NUM_SMS: tl.constexpr,  #\n                             ):\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)\n    num_tiles = num_pid_m * num_pid_n\n\n    tiles_per_SM = num_tiles // NUM_SMS\n    if start_pid < num_tiles % NUM_SMS:\n        tiles_per_SM += 1\n\n    tile_id = start_pid - NUM_SMS\n    ki = -1\n\n    offs_k_for_mask = tl.arange(0, BLOCK_SIZE_K)\n\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\n    pid_m = 0\n    pid_n = 0\n    offs_am = tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = tl.arange(0, BLOCK_SIZE_N)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for _ in range(0, k_tiles * tiles_per_SM):\n        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n        if ki == 0:\n            tile_id += NUM_SMS\n            group_id = tile_id // num_pid_in_group\n            first_pid_m = group_id * GROUP_SIZE_M\n            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n            pid_m = first_pid_m + (tile_id % group_size_m)\n            pid_n = (tile_id % num_pid_in_group) // group_size_m\n\n            start_m = pid_m * BLOCK_SIZE_M\n            start_n = pid_n * BLOCK_SIZE_N\n            offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)\n            offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)\n            offs_am = tl.where(offs_am < M, offs_am, 0)\n            offs_bn = tl.where(offs_bn < N, offs_bn, 0)\n            offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)\n            offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n        offs_k = ki * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n        a = tl.load(a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n\n        if ki == k_tiles - 1:\n            offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n            offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n            c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n            if (c_ptr.dtype.element_ty == tl.float8e4nv):\n                c = accumulator.to(tl.float8e4nv)\n            else:\n                c = accumulator.to(tl.float16)\n            tl.store(c_ptrs, c, mask=c_mask)\n            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n\ndef matmul_persistent(a, b):\n    configs = {\n        torch.float8_e4m3fn: {\n            \"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 128, \"GROUP_SIZE_M\": 8, \"num_stages\": 4,\n            \"num_warps\": 8\n        }, torch.float16: {\n            \"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 64, \"GROUP_SIZE_M\": 8, \"num_stages\": 3,\n            \"num_warps\": 8\n        }\n    }\n    # Check constraints.\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n    NUM_SMS = torch.cuda.get_device_properties(\"cuda\").multi_processor_count\n    M, K = a.shape\n    K, N = b.shape\n    dtype = a.dtype\n    # Allocates output.\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    # 1D launch kernel where each block gets its own program.\n    grid = lambda META: (min(NUM_SMS, triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"])), )\n    matmul_kernel_persistent[grid](\n        a, b, c,  #\n        M, N, K,  #\n        a.stride(0), a.stride(1),  #\n        b.stride(0), b.stride(1),  #\n        c.stride(0), c.stride(1),  #\n        BLOCK_SIZE_M=configs[dtype][\"BLOCK_SIZE_M\"],  #\n        BLOCK_SIZE_N=configs[dtype][\"BLOCK_SIZE_N\"],  #\n        BLOCK_SIZE_K=configs[dtype][\"BLOCK_SIZE_K\"],  #\n        GROUP_SIZE_M=configs[dtype][\"GROUP_SIZE_M\"],  #\n        NUM_SMS=NUM_SMS,  #\n        num_stages=configs[dtype][\"num_stages\"],  #\n        num_warps=configs[dtype][\"num_warps\"],  #\n    )\n    return c\n\n\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel_tma_persistent(a_desc_ptr, b_desc_ptr, c_desc_ptr,  #\n                                 M, N, K,  #\n                                 BLOCK_SIZE_M: tl.constexpr,  #\n                                 BLOCK_SIZE_N: tl.constexpr,  #\n                                 BLOCK_SIZE_K: tl.constexpr,  #\n                                 GROUP_SIZE_M: tl.constexpr,  #\n                                 FP8_OUTPUT: tl.constexpr,  #\n                                 NUM_SMS: tl.constexpr):  #\n    dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)\n    num_tiles = num_pid_m * num_pid_n\n\n    tiles_per_SM = num_tiles // NUM_SMS\n    if start_pid < num_tiles % NUM_SMS:\n        tiles_per_SM += 1\n\n    tile_id = start_pid - NUM_SMS\n    ki = -1\n\n    pid_m = 0\n    pid_n = 0\n    offs_am = 0\n    offs_bn = 0\n\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for _ in range(0, k_tiles * tiles_per_SM):\n        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n        if ki == 0:\n            tile_id += NUM_SMS\n            group_id = tile_id // num_pid_in_group\n            first_pid_m = group_id * GROUP_SIZE_M\n            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n            pid_m = first_pid_m + (tile_id % group_size_m)\n            pid_n = (tile_id % num_pid_in_group) // group_size_m\n\n            offs_am = pid_m * BLOCK_SIZE_M\n            offs_bn = pid_n * BLOCK_SIZE_N\n\n        offs_k = ki * BLOCK_SIZE_K\n\n        a = tl._experimental_descriptor_load(a_desc_ptr, [offs_am, offs_k], [BLOCK_SIZE_M, BLOCK_SIZE_K], dtype)\n        b = tl._experimental_descriptor_load(b_desc_ptr, [offs_bn, offs_k], [BLOCK_SIZE_N, BLOCK_SIZE_K], dtype)\n        accumulator = tl.dot(a, b.T, accumulator)\n\n        if ki == k_tiles - 1:\n            c = accumulator.to(dtype)\n\n            tl._experimental_descriptor_store(c_desc_ptr, c, [offs_am, offs_bn])\n            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n\ndef matmul_tma_persistent(a, b):\n    # Autotuner does not work with TMA. Use manual config.\n    configs = {\n        torch.float8_e4m3fn: {\n            \"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 128, \"GROUP_SIZE_M\": 8, \"num_stages\": 4,\n            \"num_warps\": 8\n        }, torch.float16: {\n            \"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 64, \"GROUP_SIZE_M\": 8, \"num_stages\": 3,\n            \"num_warps\": 8\n        }\n    }\n\n    # Check constraints.\n    assert a.shape[1] == b.shape[1], \"Incompatible dimensions\"  # b is transposed\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n\n    M, K = a.shape\n    N, K = b.shape\n    dtype = a.dtype\n\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    desc_a = triton.tools.experimental_descriptor.create_2d_tma_descriptor(a.data_ptr(), M, K,\n                                                                           configs[dtype][\"BLOCK_SIZE_M\"],\n                                                                           configs[dtype][\"BLOCK_SIZE_K\"],\n                                                                           a.element_size())\n    desc_b = triton.tools.experimental_descriptor.create_2d_tma_descriptor(b.data_ptr(), N, K,\n                                                                           configs[dtype][\"BLOCK_SIZE_N\"],\n                                                                           configs[dtype][\"BLOCK_SIZE_K\"],\n                                                                           b.element_size())\n    desc_c = triton.tools.experimental_descriptor.create_2d_tma_descriptor(c.data_ptr(), M, N,\n                                                                           configs[dtype][\"BLOCK_SIZE_M\"],\n                                                                           configs[dtype][\"BLOCK_SIZE_N\"],\n                                                                           c.element_size())\n    NUM_SMS = torch.cuda.get_device_properties(\"cuda\").multi_processor_count\n\n    grid = lambda META: (min(NUM_SMS, triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"])), )\n    matmul_kernel_tma_persistent[grid](\n        desc_a, desc_b, desc_c,  #\n        M, N, K,  #\n        BLOCK_SIZE_M=configs[dtype][\"BLOCK_SIZE_M\"],  #\n        BLOCK_SIZE_N=configs[dtype][\"BLOCK_SIZE_N\"],  #\n        BLOCK_SIZE_K=configs[dtype][\"BLOCK_SIZE_K\"],  #\n        GROUP_SIZE_M=configs[dtype][\"GROUP_SIZE_M\"],  #\n        FP8_OUTPUT=dtype == torch.float8_e4m3fn,  #\n        NUM_SMS=NUM_SMS,  #\n        num_stages=configs[dtype][\"num_stages\"],  #\n        num_warps=configs[dtype][\"num_warps\"],  #\n    )\n    return c\n\n\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel_device_tma_persistent(workspace_ptr,  #\n                                        tiles_per_update: tl.constexpr,  #\n                                        a_ptr, b_ptr, c_ptr,  #\n                                        M, N, K,  #\n                                        BLOCK_SIZE_M: tl.constexpr,  #\n                                        BLOCK_SIZE_N: tl.constexpr,  #\n                                        BLOCK_SIZE_K: tl.constexpr,  #\n                                        GROUP_SIZE_M: tl.constexpr,  #\n                                        NUM_SMS: tl.constexpr):  #\n    # Matmul using TMA and device-side descriptor creation\n    dtype = c_ptr.dtype.element_ty\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)\n    num_tiles = num_pid_m * num_pid_n\n\n    TMA_SIZE: tl.constexpr = 128\n    workspace_base = workspace_ptr + start_pid * 3 * TMA_SIZE\n    a_desc_ptr = workspace_base\n    b_desc_ptr = workspace_base + TMA_SIZE\n    c_desc_ptr = workspace_base + 2 * TMA_SIZE\n\n    tl.extra.cuda.experimental_device_tensormap_create2d(desc_ptr=a_desc_ptr, global_address=a_ptr,\n                                                         load_size=[BLOCK_SIZE_M, BLOCK_SIZE_K], global_size=[M, K],\n                                                         element_ty=a_ptr.dtype.element_ty)\n    tl.extra.cuda.experimental_device_tensormap_create2d(desc_ptr=b_desc_ptr, global_address=b_ptr,\n                                                         load_size=[BLOCK_SIZE_N, BLOCK_SIZE_K], global_size=[N, K],\n                                                         element_ty=b_ptr.dtype.element_ty)\n    tl.extra.cuda.experimental_device_tensormap_create2d(desc_ptr=c_desc_ptr, global_address=c_ptr,\n                                                         load_size=[BLOCK_SIZE_M, BLOCK_SIZE_N], global_size=[M, N],\n                                                         element_ty=c_ptr.dtype.element_ty)\n    tl.extra.cuda.experimental_tensormap_fenceproxy_acquire(a_desc_ptr)\n    tl.extra.cuda.experimental_tensormap_fenceproxy_acquire(b_desc_ptr)\n    tl.extra.cuda.experimental_tensormap_fenceproxy_acquire(c_desc_ptr)\n\n    tiles_per_SM = num_tiles // NUM_SMS\n    if start_pid < num_tiles % NUM_SMS:\n        tiles_per_SM += 1\n\n    tile_id = start_pid - NUM_SMS\n    ki = -1\n    ni = -1\n\n    pid_m = 0\n    pid_n = 0\n    offs_am = 0\n    offs_bn = 0\n\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for _ in range(0, k_tiles * tiles_per_SM):\n        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n        if ki == 0:\n            ni += 1\n\n            # Simulate a grouped gemm\n            if ni == tiles_per_update:\n                tl.extra.cuda.experimental_device_tensormap_create2d(desc_ptr=a_desc_ptr, global_address=a_ptr,\n                                                                     load_size=[BLOCK_SIZE_M,\n                                                                                BLOCK_SIZE_K], global_size=[M, K],\n                                                                     element_ty=a_ptr.dtype.element_ty)\n                tl.extra.cuda.experimental_device_tensormap_create2d(desc_ptr=b_desc_ptr, global_address=b_ptr,\n                                                                     load_size=[BLOCK_SIZE_N,\n                                                                                BLOCK_SIZE_K], global_size=[N, K],\n                                                                     element_ty=b_ptr.dtype.element_ty)\n                tl.extra.cuda.experimental_device_tensormap_create2d(desc_ptr=c_desc_ptr, global_address=c_ptr,\n                                                                     load_size=[BLOCK_SIZE_M,\n                                                                                BLOCK_SIZE_N], global_size=[M, N],\n                                                                     element_ty=c_ptr.dtype.element_ty)\n                tl.extra.cuda.experimental_tensormap_fenceproxy_acquire(a_desc_ptr)\n                tl.extra.cuda.experimental_tensormap_fenceproxy_acquire(b_desc_ptr)\n                tl.extra.cuda.experimental_tensormap_fenceproxy_acquire(c_desc_ptr)\n                ni = 0\n\n            tile_id += NUM_SMS\n            group_id = tile_id // num_pid_in_group\n            first_pid_m = group_id * GROUP_SIZE_M\n            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n            pid_m = first_pid_m + (tile_id % group_size_m)\n            pid_n = (tile_id % num_pid_in_group) // group_size_m\n\n            offs_am = pid_m * BLOCK_SIZE_M\n            offs_bn = pid_n * BLOCK_SIZE_N\n\n        offs_k = ki * BLOCK_SIZE_K\n\n        a = tl._experimental_descriptor_load(a_desc_ptr, [offs_am, offs_k], [BLOCK_SIZE_M, BLOCK_SIZE_K], dtype)\n        b = tl._experimental_descriptor_load(b_desc_ptr, [offs_bn, offs_k], [BLOCK_SIZE_N, BLOCK_SIZE_K], dtype)\n        accumulator = tl.dot(a, b.T, accumulator)\n\n        if ki == k_tiles - 1:\n            c = accumulator.to(dtype)\n\n            tl._experimental_descriptor_store(c_desc_ptr, c, [offs_am, offs_bn])\n\n            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n\ndef matmul_device_tma_persistent(a, b, tiles_per_update):\n    # Autotuner does not work with TMA. Use manual config.\n    configs = {\n        torch.float8_e4m3fn: {\n            \"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 128, \"GROUP_SIZE_M\": 8, \"num_stages\": 4,\n            \"num_warps\": 8\n        }, torch.float16: {\n            \"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 64, \"GROUP_SIZE_M\": 8, \"num_stages\": 3,\n            \"num_warps\": 8\n        }\n    }\n\n    # Check constraints.\n    assert a.shape[1] == b.shape[1], \"Incompatible dimensions\"  # b is transposed\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n\n    M, K = a.shape\n    N, K = b.shape\n    dtype = a.dtype\n\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    NUM_SMS = torch.cuda.get_device_properties(\"cuda\").multi_processor_count\n    tma_size = 128\n    workspace = torch.empty(NUM_SMS * 3 * tma_size, dtype=torch.uint8, device=\"cuda\")\n\n    grid = lambda META: (min(NUM_SMS, triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"])), )\n    matmul_kernel_device_tma_persistent[grid](\n        workspace,  #\n        tiles_per_update,  #\n        a, b, c,  #\n        M, N, K,  #\n        BLOCK_SIZE_M=configs[dtype][\"BLOCK_SIZE_M\"],  #\n        BLOCK_SIZE_N=configs[dtype][\"BLOCK_SIZE_N\"],  #\n        BLOCK_SIZE_K=configs[dtype][\"BLOCK_SIZE_K\"],  #\n        GROUP_SIZE_M=configs[dtype][\"GROUP_SIZE_M\"],  #\n        NUM_SMS=NUM_SMS,  #\n        num_stages=configs[dtype][\"num_stages\"],  #\n        num_warps=configs[dtype][\"num_warps\"],  #\n    )\n    return c\n\n\ndef cublas_matmul(a, b):\n    # Check constraints.\n    assert a.shape[1] == b.shape[1], \"Incompatible dimensions\"  # b is transposed\n    M, K = a.shape\n    N, K = b.shape\n    dtype = a.dtype\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    bytes_per_elem = a.element_size()\n    flops_str = f\"flops{bytes_per_elem * 8}\"\n    with proton.scope(f\"cublas [M={M}, N={N}, K={K}]\",\n                      {\"bytes\": bytes_per_elem * (M * K + N * K + M * N), flops_str: 2. * M * N * K}):\n        cublas.matmul(a, b, c)\n    return c\n\n\ndef torch_matmul(a, b):\n    M, K = a.shape\n    N, K = b.shape\n    bytes_per_elem = a.element_size()\n    flops_str = f\"flops{bytes_per_elem * 8}\"\n    with proton.scope(f\"torch [M={M}, N={N}, K={K}]\",\n                      {\"bytes\": bytes_per_elem * (M * K + N * K + M * N), flops_str: 2. * M * N * K}):\n        c = torch.matmul(a, b.T)\n    return c\n\n\ndef bench(K, dtype, tiles_per_update, reps=10):\n    M = 8192\n    N = 8192\n    a = torch.randn((M, K), device=\"cuda\", dtype=torch.float16).to(dtype)\n    b = torch.randn((K, N), device=\"cuda\", dtype=torch.float16).to(dtype)\n\n    b = b.T.contiguous()\n\n    proton.activate(0)\n\n    if cublas is not None:\n        for _ in range(reps):\n            cublas_matmul(a, b)\n            time.sleep(0.01)\n    if dtype == torch.float16:\n        for _ in range(reps):\n            torch_matmul(a, b)\n            time.sleep(0.01)\n    for _ in range(reps):\n        matmul(a, b.T)\n        time.sleep(0.01)\n    for _ in range(reps):\n        matmul_persistent(a, b.T)\n        time.sleep(0.01)\n    if supports_tma():\n        for _ in range(reps):\n            matmul_tma_persistent(a, b)\n            time.sleep(0.01)\n        with proton.scope(\n                f\"matmul_kernel_device_tma_persistent [M={M}, N={N}, K={K}, tiles_per_update={tiles_per_update:02}]\"):\n            for _ in range(reps):\n                matmul_device_tma_persistent(a, b, tiles_per_update)\n                time.sleep(0.01)\n\n    proton.deactivate(0)\n\n\ndef validate(M, N, K, dtype, tiles_per_update):\n    a = torch.randn((M, K), device=\"cuda\", dtype=torch.float16).to(dtype)\n    b = torch.randn((K, N), device=\"cuda\", dtype=torch.float16).to(dtype)\n    b = b.T.contiguous()\n\n    torch_result = torch_matmul(a, b) if dtype == torch.float16 else None\n    cublas_result = cublas_matmul(a, b) if cublas is not None else None\n    naive_result = matmul(a, b.T)\n    persistent_result = matmul_persistent(a, b.T)\n    tma_persistent_result = matmul_tma_persistent(a, b) if supports_tma() else None\n    device_tma_persistent_result = matmul_device_tma_persistent(a, b, tiles_per_update) if supports_tma() else None\n\n    if torch_result is not None:\n        naive_vs_torch = \"\u2705\" if torch.allclose(naive_result.to(torch.float16), torch_result.to(torch.float16),\n                                               atol=1.0) else \"\u274c\"\n    if cublas_result is not None:\n        naive_vs_cublas = \"\u2705\" if torch.allclose(naive_result.to(torch.float16), cublas_result.to(torch.float16),\n                                                atol=1.0) else \"\u274c\"\n    naive_vs_persistent = \"\u2705\" if torch.allclose(naive_result.to(torch.float16), persistent_result.to(torch.float16),\n                                                atol=1.0) else \"\u274c\"\n    if tma_persistent_result is not None:\n        naive_vs_tma_persistent = \"\u2705\" if torch.allclose(cublas_result.to(torch.float16),\n                                                        tma_persistent_result.to(torch.float16), atol=1.0) else \"\u274c\"\n    if device_tma_persistent_result is not None:\n        naive_vs_device_tma_persistent = \"\u2705\" if torch.allclose(cublas_result.to(\n            torch.float16), device_tma_persistent_result.to(torch.float16), atol=1.0) else \"\u274c\"\n    print(f\"M={M}, N={N}, K={K} verification naive vs: \", end=\"\")\n    if torch_result is not None:\n        print(f\"torch: {naive_vs_torch} \", end=\"\")\n    if cublas_result is not None:\n        print(f\"cublas: {naive_vs_cublas} \", end=\"\")\n    print(f\"persistent: {naive_vs_persistent} \", end=\"\")\n    if tma_persistent_result is not None:\n        print(f\"TMA persistent: {naive_vs_tma_persistent} \", end=\"\")\n    if device_tma_persistent_result is not None:\n        print(f\"Device TMA persistent: {naive_vs_device_tma_persistent} \", end=\"\")\n    print()\n\n\ndef show_profile(precision, profile_name):\n    import triton.profiler.viewer as proton_viewer\n    metrics = [\"time/ms\"]\n    if precision == 'fp8':\n        metrics = [\"tflop8/s\"] + metrics\n    elif precision == 'fp16':\n        metrics = [\"tflop16/s\"] + metrics\n    file_name = f\"{profile_name}.hatchet\"\n    proton_viewer.parse(metrics, file_name, depth=100)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-K\", type=int, required=False, default=512)\n    parser.add_argument(\"--K_range\", type=int, nargs=2)\n    parser.add_argument(\"--K_step\", type=int, default=512)\n    parser.add_argument(\n        \"--tiles_per_update\",\n        type=int,\n        default=1,\n        help=\n        \"Number of output tiles calculated for each update of the tma descriptor in matmul_device_tma_persistent_kernel\",\n    )\n    parser.add_argument(\"--prec\", type=str, choices=[\"fp8\", \"fp16\"], default=\"fp16\")\n    args = parser.parse_args()\n\n    if args.prec == 'fp8' and (not hasattr(torch, \"float8_e4m3fn\") or not is_cuda()):\n        print(\"This example requires CUDA with fp8 support.\")\n        exit(1)\n\n    dtype = torch.float8_e4m3fn if args.prec == 'fp8' else torch.float16\n\n    if args.K and args.K_range is None:\n        args.K_range = [args.K, args.K]\n        args.K_step = 1  # doesn't matter as long as it's not 0\n\n    torch.manual_seed(0)\n\n    validate(32, 32, 32, dtype, args.tiles_per_update)\n    validate(8192, 8192, 512, dtype, args.tiles_per_update)\n\n    proton.start(\"matmul\", hook=\"triton\")\n    for K in range(args.K_range[0], args.K_range[1] + 1, args.K_step):\n        bench(K, dtype, args.tiles_per_update)\n    proton.finalize()\n    show_profile(args.prec, \"matmul\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}